{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob  # to pick random image from test\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "from data.data_loading import TRANSFORM, create_angle_features\n",
    "from pose.pose_utils import TESTPATH, CLASS_MAPPINGS_NAMES, LANDMARKS_ANGLES_DICT, LANDMARK_DICT, calc_limb_lengths\n",
    "from pose.plot import plot_image, plot_3d_keypoints, plot_distribution_with_image\n",
    "from pose.pose_mediapipe import pose_landmarks_to_list, estimate_poses\n",
    "from classifier.classify import classify_image\n",
    "from pose.nearest_neighbor_correction import get_angle_confidence_intervals, create_pose_df, warrior_pose_front_back, \\\n",
    "get_annotated_img, compare_two_figures, select_correct_closest_image, which_leg_front, plot_3D, normalize_on_right_hip\n",
    "from pose.decision_tree import decision_tree_correct\n",
    "from gan.results_cLimbGAN import generate_coords_given_limb_lengths\n",
    "from classifier.classify_pose_quality import classify_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03c035",
   "metadata": {},
   "source": [
    "## Read in Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14941d2e",
   "metadata": {},
   "source": [
    "Note that these images are all unseen by the system in training (ie test images). They can be images that are correct or incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a25ef0",
   "metadata": {},
   "source": [
    "You can put in your own photos in the `data/test/` folder and it will get randomly picked by the code snippet below. If you wish, you can also manually set the path to your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4c78e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Selects a random image from the test folder\n",
    "path = str(TESTPATH)\n",
    "types = ('*.jpg','*.jpeg')\n",
    "tests_images = []\n",
    "for files in types:\n",
    "    tests_images.extend(glob.glob(os.path.join(TESTPATH,'**',files),recursive=True))\n",
    "CPATH = np.random.choice(tests_images)\n",
    "test_image = TRANSFORM(Image.open(CPATH))\n",
    "plot_image(test_image, dataloader=True)\n",
    "\n",
    "# Prints human labeled characteristics of the pose\n",
    "secret_truth = CPATH.split('/')[-2].split('_')\n",
    "print(f\"Ground Truth: \\nThis is a {secret_truth[1]} pose\")\n",
    "if secret_truth[0] == '0':\n",
    "    print('This pose is an INCORRECT pose')\n",
    "else:\n",
    "    print('This pose is a Correct pose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ecb125",
   "metadata": {},
   "source": [
    "### Apply Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5adc050",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result, annotated_test_image = estimate_poses(test_image, CPATH, skip_image_annotation=False)\n",
    "plot_image(annotated_test_image, dataloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val, _, nump = pose_landmarks_to_list(test_result, 'pose_world_landmarks')\n",
    "\n",
    "df_test = pd.DataFrame.from_records([val]).rename(LANDMARK_DICT, axis=1)\n",
    "create_angle_features(df_test)\n",
    "\n",
    "np_test = normalize_on_right_hip(np.array([nump]))\n",
    "\n",
    "x = np_test[0].T[0]\n",
    "y = np_test[0].T[1]\n",
    "z = np_test[0].T[2]\n",
    "\n",
    "plot_3d_keypoints(x, y, z, -70, 270)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261be6a",
   "metadata": {},
   "source": [
    "### Pose Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = classify_image(np_test).item()\n",
    "print(f\"Image Classified as : {CLASS_MAPPINGS_NAMES[label]}\")\n",
    "correct = classify_correct(np_test).item()\n",
    "if correct < .5:\n",
    "    print(f\"Image is a Bad Pose\")\n",
    "else:\n",
    "    print(\"Image is a Good Pose\")\n",
    "    \n",
    "df_w1, df_w2, df_dd = create_pose_df()\n",
    "df_test_handedness = df_test.copy()\n",
    "\n",
    "if label == 0:\n",
    "    df = df_dd\n",
    "    LANDMARKS = LANDMARKS_ANGLES_DICT.keys()\n",
    "elif label == 1:\n",
    "    df, LANDMARKS = warrior_pose_front_back(df_w1)\n",
    "    df_test = which_leg_front(df_test)\n",
    "elif label == 2:\n",
    "    df, LANDMARKS = warrior_pose_front_back(df_w2)\n",
    "    df_test = which_leg_front(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaf3e9",
   "metadata": {},
   "source": [
    "# Pose Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e63b52",
   "metadata": {},
   "source": [
    "### The Learned Angle Distribution Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0328c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_angles = get_angle_confidence_intervals(df, LANDMARKS, percent = .15)\n",
    "plot_distribution_with_image(df, df_test, distribution_angles, LANDMARKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ccdf6",
   "metadata": {},
   "source": [
    "### The Nearest Neighbour Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d0fc4",
   "metadata": {},
   "source": [
    "The image on the left is the input image, the image on the right is the closest correct nearest neighbor.\n",
    "\n",
    "Red is the original image, green is the output correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad96124",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth, ground_truth_indx = select_correct_closest_image(np_test, df)\n",
    "ground_truth_img = get_annotated_img(ground_truth_indx)\n",
    "\n",
    "compare_two_figures(np.squeeze(np_test), np.squeeze(ground_truth), annotated_test_image, ground_truth_img, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddac3e",
   "metadata": {},
   "source": [
    "### The Generative GAN Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9002f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_coords_given_limb_lengths(calc_limb_lengths(np.squeeze(np_test, axis=0)), label, version=780, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
